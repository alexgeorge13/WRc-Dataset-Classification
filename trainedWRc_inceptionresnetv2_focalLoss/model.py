#    This file was created by
#    MATLAB Deep Learning Toolbox Converter for TensorFlow Models.
#    10-Jul-2025 12:33:19

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def create_model():
    input_1_unnormalized = keras.Input(shape=(299,299,3), name="input_1_unnormalized")
    input_1 = keras.layers.Normalization(axis=(1,2,3), name="input_1_")(input_1_unnormalized)
    conv2d_1 = layers.Conv2D(32, (3,3), strides=(2,2), name="conv2d_1_")(input_1)
    batch_normalization_1 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_1_")(conv2d_1)
    activation_1 = layers.ReLU()(batch_normalization_1)
    conv2d_2 = layers.Conv2D(32, (3,3), name="conv2d_2_")(activation_1)
    batch_normalization_2 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_2_")(conv2d_2)
    activation_2 = layers.ReLU()(batch_normalization_2)
    conv2d_3 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_3_")(activation_2)
    batch_normalization_3 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_3_")(conv2d_3)
    activation_3 = layers.ReLU()(batch_normalization_3)
    max_pooling2d_1 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(activation_3)
    conv2d_4 = layers.Conv2D(80, (1,1), name="conv2d_4_")(max_pooling2d_1)
    batch_normalization_4 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_4_")(conv2d_4)
    activation_4 = layers.ReLU()(batch_normalization_4)
    conv2d_5 = layers.Conv2D(192, (3,3), name="conv2d_5_")(activation_4)
    batch_normalization_5 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_5_")(conv2d_5)
    activation_5 = layers.ReLU()(batch_normalization_5)
    max_pooling2d_2 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(activation_5)
    conv2d_9 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_9_")(max_pooling2d_2)
    batch_normalization_9 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_9_")(conv2d_9)
    activation_9 = layers.ReLU()(batch_normalization_9)
    conv2d_7 = layers.Conv2D(48, (1,1), padding="same", name="conv2d_7_")(max_pooling2d_2)
    conv2d_10 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_10_")(activation_9)
    batch_normalization_7 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_7_")(conv2d_7)
    batch_normalization_10 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_10_")(conv2d_10)
    activation_7 = layers.ReLU()(batch_normalization_7)
    activation_10 = layers.ReLU()(batch_normalization_10)
    average_pooling2d_1_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(max_pooling2d_2)
    average_pooling2d_1 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_1_prepadded)
    conv2d_6 = layers.Conv2D(96, (1,1), padding="same", name="conv2d_6_")(max_pooling2d_2)
    conv2d_8 = layers.Conv2D(64, (5,5), padding="same", name="conv2d_8_")(activation_7)
    conv2d_11 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_11_")(activation_10)
    conv2d_12 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_12_")(average_pooling2d_1)
    batch_normalization_6 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_6_")(conv2d_6)
    batch_normalization_8 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_8_")(conv2d_8)
    batch_normalization_11 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_11_")(conv2d_11)
    batch_normalization_12 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_12_")(conv2d_12)
    activation_6 = layers.ReLU()(batch_normalization_6)
    activation_8 = layers.ReLU()(batch_normalization_8)
    activation_11 = layers.ReLU()(batch_normalization_11)
    activation_12 = layers.ReLU()(batch_normalization_12)
    mixed_5b = layers.Concatenate(axis=-1)([activation_6, activation_8, activation_11, activation_12])
    conv2d_16 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_16_")(mixed_5b)
    batch_normalization_16 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_16_")(conv2d_16)
    activation_16 = layers.ReLU()(batch_normalization_16)
    conv2d_14 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_14_")(mixed_5b)
    conv2d_17 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_17_")(activation_16)
    batch_normalization_14 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_14_")(conv2d_14)
    batch_normalization_17 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_17_")(conv2d_17)
    activation_14 = layers.ReLU()(batch_normalization_14)
    activation_17 = layers.ReLU()(batch_normalization_17)
    conv2d_13 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_13_")(mixed_5b)
    conv2d_15 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_15_")(activation_14)
    conv2d_18 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_18_")(activation_17)
    batch_normalization_13 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_13_")(conv2d_13)
    batch_normalization_15 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_15_")(conv2d_15)
    batch_normalization_18 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_18_")(conv2d_18)
    activation_13 = layers.ReLU()(batch_normalization_13)
    activation_15 = layers.ReLU()(batch_normalization_15)
    activation_18 = layers.ReLU()(batch_normalization_18)
    block35_1_mixed = layers.Concatenate(axis=-1)([activation_13, activation_15, activation_18])
    block35_1_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_1_conv_")(block35_1_mixed)
    block35_1_scale = layers.Lambda(lambda x: x * 0.170000)(block35_1_conv)
    block35_1 = layers.Add()([mixed_5b, block35_1_scale])
    block35_1_ac = layers.ReLU()(block35_1)
    conv2d_22 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_22_")(block35_1_ac)
    batch_normalization_22 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_22_")(conv2d_22)
    activation_22 = layers.ReLU()(batch_normalization_22)
    conv2d_20 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_20_")(block35_1_ac)
    conv2d_23 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_23_")(activation_22)
    batch_normalization_20 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_20_")(conv2d_20)
    batch_normalization_23 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_23_")(conv2d_23)
    activation_20 = layers.ReLU()(batch_normalization_20)
    activation_23 = layers.ReLU()(batch_normalization_23)
    conv2d_19 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_19_")(block35_1_ac)
    conv2d_21 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_21_")(activation_20)
    conv2d_24 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_24_")(activation_23)
    batch_normalization_19 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_19_")(conv2d_19)
    batch_normalization_21 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_21_")(conv2d_21)
    batch_normalization_24 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_24_")(conv2d_24)
    activation_19 = layers.ReLU()(batch_normalization_19)
    activation_21 = layers.ReLU()(batch_normalization_21)
    activation_24 = layers.ReLU()(batch_normalization_24)
    block35_2_mixed = layers.Concatenate(axis=-1)([activation_19, activation_21, activation_24])
    block35_2_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_2_conv_")(block35_2_mixed)
    block35_2_scale = layers.Lambda(lambda x: x * 0.170000)(block35_2_conv)
    block35_2 = layers.Add()([block35_1_ac, block35_2_scale])
    block35_2_ac = layers.ReLU()(block35_2)
    conv2d_28 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_28_")(block35_2_ac)
    batch_normalization_28 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_28_")(conv2d_28)
    activation_28 = layers.ReLU()(batch_normalization_28)
    conv2d_26 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_26_")(block35_2_ac)
    conv2d_29 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_29_")(activation_28)
    batch_normalization_26 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_26_")(conv2d_26)
    batch_normalization_29 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_29_")(conv2d_29)
    activation_26 = layers.ReLU()(batch_normalization_26)
    activation_29 = layers.ReLU()(batch_normalization_29)
    conv2d_25 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_25_")(block35_2_ac)
    conv2d_27 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_27_")(activation_26)
    conv2d_30 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_30_")(activation_29)
    batch_normalization_25 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_25_")(conv2d_25)
    batch_normalization_27 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_27_")(conv2d_27)
    batch_normalization_30 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_30_")(conv2d_30)
    activation_25 = layers.ReLU()(batch_normalization_25)
    activation_27 = layers.ReLU()(batch_normalization_27)
    activation_30 = layers.ReLU()(batch_normalization_30)
    block35_3_mixed = layers.Concatenate(axis=-1)([activation_25, activation_27, activation_30])
    block35_3_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_3_conv_")(block35_3_mixed)
    block35_3_scale = layers.Lambda(lambda x: x * 0.170000)(block35_3_conv)
    block35_3 = layers.Add()([block35_2_ac, block35_3_scale])
    block35_3_ac = layers.ReLU()(block35_3)
    conv2d_34 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_34_")(block35_3_ac)
    batch_normalization_34 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_34_")(conv2d_34)
    activation_34 = layers.ReLU()(batch_normalization_34)
    conv2d_32 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_32_")(block35_3_ac)
    conv2d_35 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_35_")(activation_34)
    batch_normalization_32 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_32_")(conv2d_32)
    batch_normalization_35 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_35_")(conv2d_35)
    activation_32 = layers.ReLU()(batch_normalization_32)
    activation_35 = layers.ReLU()(batch_normalization_35)
    conv2d_31 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_31_")(block35_3_ac)
    conv2d_33 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_33_")(activation_32)
    conv2d_36 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_36_")(activation_35)
    batch_normalization_31 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_31_")(conv2d_31)
    batch_normalization_33 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_33_")(conv2d_33)
    batch_normalization_36 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_36_")(conv2d_36)
    activation_31 = layers.ReLU()(batch_normalization_31)
    activation_33 = layers.ReLU()(batch_normalization_33)
    activation_36 = layers.ReLU()(batch_normalization_36)
    block35_4_mixed = layers.Concatenate(axis=-1)([activation_31, activation_33, activation_36])
    block35_4_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_4_conv_")(block35_4_mixed)
    block35_4_scale = layers.Lambda(lambda x: x * 0.170000)(block35_4_conv)
    block35_4 = layers.Add()([block35_3_ac, block35_4_scale])
    block35_4_ac = layers.ReLU()(block35_4)
    conv2d_40 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_40_")(block35_4_ac)
    batch_normalization_40 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_40_")(conv2d_40)
    activation_40 = layers.ReLU()(batch_normalization_40)
    conv2d_38 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_38_")(block35_4_ac)
    conv2d_41 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_41_")(activation_40)
    batch_normalization_38 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_38_")(conv2d_38)
    batch_normalization_41 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_41_")(conv2d_41)
    activation_38 = layers.ReLU()(batch_normalization_38)
    activation_41 = layers.ReLU()(batch_normalization_41)
    conv2d_37 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_37_")(block35_4_ac)
    conv2d_39 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_39_")(activation_38)
    conv2d_42 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_42_")(activation_41)
    batch_normalization_37 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_37_")(conv2d_37)
    batch_normalization_39 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_39_")(conv2d_39)
    batch_normalization_42 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_42_")(conv2d_42)
    activation_37 = layers.ReLU()(batch_normalization_37)
    activation_39 = layers.ReLU()(batch_normalization_39)
    activation_42 = layers.ReLU()(batch_normalization_42)
    block35_5_mixed = layers.Concatenate(axis=-1)([activation_37, activation_39, activation_42])
    block35_5_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_5_conv_")(block35_5_mixed)
    block35_5_scale = layers.Lambda(lambda x: x * 0.170000)(block35_5_conv)
    block35_5 = layers.Add()([block35_4_ac, block35_5_scale])
    block35_5_ac = layers.ReLU()(block35_5)
    conv2d_46 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_46_")(block35_5_ac)
    batch_normalization_46 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_46_")(conv2d_46)
    activation_46 = layers.ReLU()(batch_normalization_46)
    conv2d_44 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_44_")(block35_5_ac)
    conv2d_47 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_47_")(activation_46)
    batch_normalization_44 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_44_")(conv2d_44)
    batch_normalization_47 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_47_")(conv2d_47)
    activation_44 = layers.ReLU()(batch_normalization_44)
    activation_47 = layers.ReLU()(batch_normalization_47)
    conv2d_43 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_43_")(block35_5_ac)
    conv2d_45 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_45_")(activation_44)
    conv2d_48 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_48_")(activation_47)
    batch_normalization_43 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_43_")(conv2d_43)
    batch_normalization_45 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_45_")(conv2d_45)
    batch_normalization_48 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_48_")(conv2d_48)
    activation_43 = layers.ReLU()(batch_normalization_43)
    activation_45 = layers.ReLU()(batch_normalization_45)
    activation_48 = layers.ReLU()(batch_normalization_48)
    block35_6_mixed = layers.Concatenate(axis=-1)([activation_43, activation_45, activation_48])
    block35_6_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_6_conv_")(block35_6_mixed)
    block35_6_scale = layers.Lambda(lambda x: x * 0.170000)(block35_6_conv)
    block35_6 = layers.Add()([block35_5_ac, block35_6_scale])
    block35_6_ac = layers.ReLU()(block35_6)
    conv2d_52 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_52_")(block35_6_ac)
    batch_normalization_52 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_52_")(conv2d_52)
    activation_52 = layers.ReLU()(batch_normalization_52)
    conv2d_50 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_50_")(block35_6_ac)
    conv2d_53 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_53_")(activation_52)
    batch_normalization_50 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_50_")(conv2d_50)
    batch_normalization_53 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_53_")(conv2d_53)
    activation_50 = layers.ReLU()(batch_normalization_50)
    activation_53 = layers.ReLU()(batch_normalization_53)
    conv2d_49 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_49_")(block35_6_ac)
    conv2d_51 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_51_")(activation_50)
    conv2d_54 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_54_")(activation_53)
    batch_normalization_49 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_49_")(conv2d_49)
    batch_normalization_51 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_51_")(conv2d_51)
    batch_normalization_54 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_54_")(conv2d_54)
    activation_49 = layers.ReLU()(batch_normalization_49)
    activation_51 = layers.ReLU()(batch_normalization_51)
    activation_54 = layers.ReLU()(batch_normalization_54)
    block35_7_mixed = layers.Concatenate(axis=-1)([activation_49, activation_51, activation_54])
    block35_7_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_7_conv_")(block35_7_mixed)
    block35_7_scale = layers.Lambda(lambda x: x * 0.170000)(block35_7_conv)
    block35_7 = layers.Add()([block35_6_ac, block35_7_scale])
    block35_7_ac = layers.ReLU()(block35_7)
    conv2d_58 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_58_")(block35_7_ac)
    batch_normalization_58 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_58_")(conv2d_58)
    activation_58 = layers.ReLU()(batch_normalization_58)
    conv2d_56 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_56_")(block35_7_ac)
    conv2d_59 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_59_")(activation_58)
    batch_normalization_56 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_56_")(conv2d_56)
    batch_normalization_59 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_59_")(conv2d_59)
    activation_56 = layers.ReLU()(batch_normalization_56)
    activation_59 = layers.ReLU()(batch_normalization_59)
    conv2d_55 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_55_")(block35_7_ac)
    conv2d_57 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_57_")(activation_56)
    conv2d_60 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_60_")(activation_59)
    batch_normalization_55 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_55_")(conv2d_55)
    batch_normalization_57 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_57_")(conv2d_57)
    batch_normalization_60 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_60_")(conv2d_60)
    activation_55 = layers.ReLU()(batch_normalization_55)
    activation_57 = layers.ReLU()(batch_normalization_57)
    activation_60 = layers.ReLU()(batch_normalization_60)
    block35_8_mixed = layers.Concatenate(axis=-1)([activation_55, activation_57, activation_60])
    block35_8_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_8_conv_")(block35_8_mixed)
    block35_8_scale = layers.Lambda(lambda x: x * 0.170000)(block35_8_conv)
    block35_8 = layers.Add()([block35_7_ac, block35_8_scale])
    block35_8_ac = layers.ReLU()(block35_8)
    conv2d_64 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_64_")(block35_8_ac)
    batch_normalization_64 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_64_")(conv2d_64)
    activation_64 = layers.ReLU()(batch_normalization_64)
    conv2d_62 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_62_")(block35_8_ac)
    conv2d_65 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_65_")(activation_64)
    batch_normalization_62 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_62_")(conv2d_62)
    batch_normalization_65 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_65_")(conv2d_65)
    activation_62 = layers.ReLU()(batch_normalization_62)
    activation_65 = layers.ReLU()(batch_normalization_65)
    conv2d_61 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_61_")(block35_8_ac)
    conv2d_63 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_63_")(activation_62)
    conv2d_66 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_66_")(activation_65)
    batch_normalization_61 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_61_")(conv2d_61)
    batch_normalization_63 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_63_")(conv2d_63)
    batch_normalization_66 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_66_")(conv2d_66)
    activation_61 = layers.ReLU()(batch_normalization_61)
    activation_63 = layers.ReLU()(batch_normalization_63)
    activation_66 = layers.ReLU()(batch_normalization_66)
    block35_9_mixed = layers.Concatenate(axis=-1)([activation_61, activation_63, activation_66])
    block35_9_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_9_conv_")(block35_9_mixed)
    block35_9_scale = layers.Lambda(lambda x: x * 0.170000)(block35_9_conv)
    block35_9 = layers.Add()([block35_8_ac, block35_9_scale])
    block35_9_ac = layers.ReLU()(block35_9)
    conv2d_70 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_70_")(block35_9_ac)
    batch_normalization_70 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_70_")(conv2d_70)
    activation_70 = layers.ReLU()(batch_normalization_70)
    conv2d_68 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_68_")(block35_9_ac)
    conv2d_71 = layers.Conv2D(48, (3,3), padding="same", name="conv2d_71_")(activation_70)
    batch_normalization_68 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_68_")(conv2d_68)
    batch_normalization_71 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_71_")(conv2d_71)
    activation_68 = layers.ReLU()(batch_normalization_68)
    activation_71 = layers.ReLU()(batch_normalization_71)
    conv2d_67 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_67_")(block35_9_ac)
    conv2d_69 = layers.Conv2D(32, (3,3), padding="same", name="conv2d_69_")(activation_68)
    conv2d_72 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_72_")(activation_71)
    batch_normalization_67 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_67_")(conv2d_67)
    batch_normalization_69 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_69_")(conv2d_69)
    batch_normalization_72 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_72_")(conv2d_72)
    activation_67 = layers.ReLU()(batch_normalization_67)
    activation_69 = layers.ReLU()(batch_normalization_69)
    activation_72 = layers.ReLU()(batch_normalization_72)
    block35_10_mixed = layers.Concatenate(axis=-1)([activation_67, activation_69, activation_72])
    block35_10_conv = layers.Conv2D(320, (1,1), padding="same", name="block35_10_conv_")(block35_10_mixed)
    block35_10_scale = layers.Lambda(lambda x: x * 0.170000)(block35_10_conv)
    block35_10 = layers.Add()([block35_9_ac, block35_10_scale])
    block35_10_ac = layers.ReLU()(block35_10)
    conv2d_74 = layers.Conv2D(256, (1,1), padding="same", name="conv2d_74_")(block35_10_ac)
    batch_normalization_74 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_74_")(conv2d_74)
    activation_74 = layers.ReLU()(batch_normalization_74)
    conv2d_75 = layers.Conv2D(256, (3,3), padding="same", name="conv2d_75_")(activation_74)
    batch_normalization_75 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_75_")(conv2d_75)
    activation_75 = layers.ReLU()(batch_normalization_75)
    conv2d_73 = layers.Conv2D(384, (3,3), strides=(2,2), name="conv2d_73_")(block35_10_ac)
    conv2d_76 = layers.Conv2D(384, (3,3), strides=(2,2), name="conv2d_76_")(activation_75)
    batch_normalization_73 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_73_")(conv2d_73)
    batch_normalization_76 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_76_")(conv2d_76)
    activation_73 = layers.ReLU()(batch_normalization_73)
    activation_76 = layers.ReLU()(batch_normalization_76)
    max_pooling2d_3 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(block35_10_ac)
    mixed_6a = layers.Concatenate(axis=-1)([activation_73, activation_76, max_pooling2d_3])
    conv2d_78 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_78_")(mixed_6a)
    batch_normalization_78 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_78_")(conv2d_78)
    activation_78 = layers.ReLU()(batch_normalization_78)
    conv2d_79 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_79_")(activation_78)
    batch_normalization_79 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_79_")(conv2d_79)
    activation_79 = layers.ReLU()(batch_normalization_79)
    conv2d_77 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_77_")(mixed_6a)
    conv2d_80 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_80_")(activation_79)
    batch_normalization_77 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_77_")(conv2d_77)
    batch_normalization_80 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_80_")(conv2d_80)
    activation_77 = layers.ReLU()(batch_normalization_77)
    activation_80 = layers.ReLU()(batch_normalization_80)
    block17_1_mixed = layers.Concatenate(axis=-1)([activation_77, activation_80])
    block17_1_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_1_conv_")(block17_1_mixed)
    block17_1_scale = layers.Lambda(lambda x: x * 0.100000)(block17_1_conv)
    block17_1 = layers.Add()([mixed_6a, block17_1_scale])
    block17_1_ac = layers.ReLU()(block17_1)
    conv2d_82 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_82_")(block17_1_ac)
    batch_normalization_82 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_82_")(conv2d_82)
    activation_82 = layers.ReLU()(batch_normalization_82)
    conv2d_83 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_83_")(activation_82)
    batch_normalization_83 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_83_")(conv2d_83)
    activation_83 = layers.ReLU()(batch_normalization_83)
    conv2d_81 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_81_")(block17_1_ac)
    conv2d_84 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_84_")(activation_83)
    batch_normalization_81 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_81_")(conv2d_81)
    batch_normalization_84 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_84_")(conv2d_84)
    activation_81 = layers.ReLU()(batch_normalization_81)
    activation_84 = layers.ReLU()(batch_normalization_84)
    block17_2_mixed = layers.Concatenate(axis=-1)([activation_81, activation_84])
    block17_2_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_2_conv_")(block17_2_mixed)
    block17_2_scale = layers.Lambda(lambda x: x * 0.100000)(block17_2_conv)
    block17_2 = layers.Add()([block17_1_ac, block17_2_scale])
    block17_2_ac = layers.ReLU()(block17_2)
    conv2d_86 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_86_")(block17_2_ac)
    batch_normalization_86 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_86_")(conv2d_86)
    activation_86 = layers.ReLU()(batch_normalization_86)
    conv2d_87 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_87_")(activation_86)
    batch_normalization_87 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_87_")(conv2d_87)
    activation_87 = layers.ReLU()(batch_normalization_87)
    conv2d_85 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_85_")(block17_2_ac)
    conv2d_88 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_88_")(activation_87)
    batch_normalization_85 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_85_")(conv2d_85)
    batch_normalization_88 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_88_")(conv2d_88)
    activation_85 = layers.ReLU()(batch_normalization_85)
    activation_88 = layers.ReLU()(batch_normalization_88)
    block17_3_mixed = layers.Concatenate(axis=-1)([activation_85, activation_88])
    block17_3_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_3_conv_")(block17_3_mixed)
    block17_3_scale = layers.Lambda(lambda x: x * 0.100000)(block17_3_conv)
    block17_3 = layers.Add()([block17_2_ac, block17_3_scale])
    block17_3_ac = layers.ReLU()(block17_3)
    conv2d_90 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_90_")(block17_3_ac)
    batch_normalization_90 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_90_")(conv2d_90)
    activation_90 = layers.ReLU()(batch_normalization_90)
    conv2d_91 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_91_")(activation_90)
    batch_normalization_91 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_91_")(conv2d_91)
    activation_91 = layers.ReLU()(batch_normalization_91)
    conv2d_89 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_89_")(block17_3_ac)
    conv2d_92 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_92_")(activation_91)
    batch_normalization_89 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_89_")(conv2d_89)
    batch_normalization_92 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_92_")(conv2d_92)
    activation_89 = layers.ReLU()(batch_normalization_89)
    activation_92 = layers.ReLU()(batch_normalization_92)
    block17_4_mixed = layers.Concatenate(axis=-1)([activation_89, activation_92])
    block17_4_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_4_conv_")(block17_4_mixed)
    block17_4_scale = layers.Lambda(lambda x: x * 0.100000)(block17_4_conv)
    block17_4 = layers.Add()([block17_3_ac, block17_4_scale])
    block17_4_ac = layers.ReLU()(block17_4)
    conv2d_94 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_94_")(block17_4_ac)
    batch_normalization_94 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_94_")(conv2d_94)
    activation_94 = layers.ReLU()(batch_normalization_94)
    conv2d_95 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_95_")(activation_94)
    batch_normalization_95 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_95_")(conv2d_95)
    activation_95 = layers.ReLU()(batch_normalization_95)
    conv2d_93 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_93_")(block17_4_ac)
    conv2d_96 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_96_")(activation_95)
    batch_normalization_93 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_93_")(conv2d_93)
    batch_normalization_96 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_96_")(conv2d_96)
    activation_93 = layers.ReLU()(batch_normalization_93)
    activation_96 = layers.ReLU()(batch_normalization_96)
    block17_5_mixed = layers.Concatenate(axis=-1)([activation_93, activation_96])
    block17_5_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_5_conv_")(block17_5_mixed)
    block17_5_scale = layers.Lambda(lambda x: x * 0.100000)(block17_5_conv)
    block17_5 = layers.Add()([block17_4_ac, block17_5_scale])
    block17_5_ac = layers.ReLU()(block17_5)
    conv2d_98 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_98_")(block17_5_ac)
    batch_normalization_98 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_98_")(conv2d_98)
    activation_98 = layers.ReLU()(batch_normalization_98)
    conv2d_99 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_99_")(activation_98)
    batch_normalization_99 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_99_")(conv2d_99)
    activation_99 = layers.ReLU()(batch_normalization_99)
    conv2d_97 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_97_")(block17_5_ac)
    conv2d_100 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_100_")(activation_99)
    batch_normalization_97 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_97_")(conv2d_97)
    batch_normalization_100 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_100_")(conv2d_100)
    activation_97 = layers.ReLU()(batch_normalization_97)
    activation_100 = layers.ReLU()(batch_normalization_100)
    block17_6_mixed = layers.Concatenate(axis=-1)([activation_97, activation_100])
    block17_6_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_6_conv_")(block17_6_mixed)
    block17_6_scale = layers.Lambda(lambda x: x * 0.100000)(block17_6_conv)
    block17_6 = layers.Add()([block17_5_ac, block17_6_scale])
    block17_6_ac = layers.ReLU()(block17_6)
    conv2d_102 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_102_")(block17_6_ac)
    batch_normalization_102 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_102_")(conv2d_102)
    activation_102 = layers.ReLU()(batch_normalization_102)
    conv2d_103 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_103_")(activation_102)
    batch_normalization_103 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_103_")(conv2d_103)
    activation_103 = layers.ReLU()(batch_normalization_103)
    conv2d_101 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_101_")(block17_6_ac)
    conv2d_104 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_104_")(activation_103)
    batch_normalization_101 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_101_")(conv2d_101)
    batch_normalization_104 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_104_")(conv2d_104)
    activation_101 = layers.ReLU()(batch_normalization_101)
    activation_104 = layers.ReLU()(batch_normalization_104)
    block17_7_mixed = layers.Concatenate(axis=-1)([activation_101, activation_104])
    block17_7_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_7_conv_")(block17_7_mixed)
    block17_7_scale = layers.Lambda(lambda x: x * 0.100000)(block17_7_conv)
    block17_7 = layers.Add()([block17_6_ac, block17_7_scale])
    block17_7_ac = layers.ReLU()(block17_7)
    conv2d_106 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_106_")(block17_7_ac)
    batch_normalization_106 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_106_")(conv2d_106)
    activation_106 = layers.ReLU()(batch_normalization_106)
    conv2d_107 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_107_")(activation_106)
    batch_normalization_107 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_107_")(conv2d_107)
    activation_107 = layers.ReLU()(batch_normalization_107)
    conv2d_105 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_105_")(block17_7_ac)
    conv2d_108 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_108_")(activation_107)
    batch_normalization_105 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_105_")(conv2d_105)
    batch_normalization_108 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_108_")(conv2d_108)
    activation_105 = layers.ReLU()(batch_normalization_105)
    activation_108 = layers.ReLU()(batch_normalization_108)
    block17_8_mixed = layers.Concatenate(axis=-1)([activation_105, activation_108])
    block17_8_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_8_conv_")(block17_8_mixed)
    block17_8_scale = layers.Lambda(lambda x: x * 0.100000)(block17_8_conv)
    block17_8 = layers.Add()([block17_7_ac, block17_8_scale])
    block17_8_ac = layers.ReLU()(block17_8)
    conv2d_110 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_110_")(block17_8_ac)
    batch_normalization_110 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_110_")(conv2d_110)
    activation_110 = layers.ReLU()(batch_normalization_110)
    conv2d_111 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_111_")(activation_110)
    batch_normalization_111 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_111_")(conv2d_111)
    activation_111 = layers.ReLU()(batch_normalization_111)
    conv2d_109 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_109_")(block17_8_ac)
    conv2d_112 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_112_")(activation_111)
    batch_normalization_109 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_109_")(conv2d_109)
    batch_normalization_112 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_112_")(conv2d_112)
    activation_109 = layers.ReLU()(batch_normalization_109)
    activation_112 = layers.ReLU()(batch_normalization_112)
    block17_9_mixed = layers.Concatenate(axis=-1)([activation_109, activation_112])
    block17_9_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_9_conv_")(block17_9_mixed)
    block17_9_scale = layers.Lambda(lambda x: x * 0.100000)(block17_9_conv)
    block17_9 = layers.Add()([block17_8_ac, block17_9_scale])
    block17_9_ac = layers.ReLU()(block17_9)
    conv2d_114 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_114_")(block17_9_ac)
    batch_normalization_114 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_114_")(conv2d_114)
    activation_114 = layers.ReLU()(batch_normalization_114)
    conv2d_115 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_115_")(activation_114)
    batch_normalization_115 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_115_")(conv2d_115)
    activation_115 = layers.ReLU()(batch_normalization_115)
    conv2d_113 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_113_")(block17_9_ac)
    conv2d_116 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_116_")(activation_115)
    batch_normalization_113 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_113_")(conv2d_113)
    batch_normalization_116 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_116_")(conv2d_116)
    activation_113 = layers.ReLU()(batch_normalization_113)
    activation_116 = layers.ReLU()(batch_normalization_116)
    block17_10_mixed = layers.Concatenate(axis=-1)([activation_113, activation_116])
    block17_10_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_10_conv_")(block17_10_mixed)
    block17_10_scale = layers.Lambda(lambda x: x * 0.100000)(block17_10_conv)
    block17_10 = layers.Add()([block17_9_ac, block17_10_scale])
    block17_10_ac = layers.ReLU()(block17_10)
    conv2d_118 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_118_")(block17_10_ac)
    batch_normalization_118 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_118_")(conv2d_118)
    activation_118 = layers.ReLU()(batch_normalization_118)
    conv2d_119 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_119_")(activation_118)
    batch_normalization_119 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_119_")(conv2d_119)
    activation_119 = layers.ReLU()(batch_normalization_119)
    conv2d_117 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_117_")(block17_10_ac)
    conv2d_120 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_120_")(activation_119)
    batch_normalization_117 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_117_")(conv2d_117)
    batch_normalization_120 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_120_")(conv2d_120)
    activation_117 = layers.ReLU()(batch_normalization_117)
    activation_120 = layers.ReLU()(batch_normalization_120)
    block17_11_mixed = layers.Concatenate(axis=-1)([activation_117, activation_120])
    block17_11_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_11_conv_")(block17_11_mixed)
    block17_11_scale = layers.Lambda(lambda x: x * 0.100000)(block17_11_conv)
    block17_11 = layers.Add()([block17_10_ac, block17_11_scale])
    block17_11_ac = layers.ReLU()(block17_11)
    conv2d_122 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_122_")(block17_11_ac)
    batch_normalization_122 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_122_")(conv2d_122)
    activation_122 = layers.ReLU()(batch_normalization_122)
    conv2d_123 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_123_")(activation_122)
    batch_normalization_123 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_123_")(conv2d_123)
    activation_123 = layers.ReLU()(batch_normalization_123)
    conv2d_121 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_121_")(block17_11_ac)
    conv2d_124 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_124_")(activation_123)
    batch_normalization_121 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_121_")(conv2d_121)
    batch_normalization_124 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_124_")(conv2d_124)
    activation_121 = layers.ReLU()(batch_normalization_121)
    activation_124 = layers.ReLU()(batch_normalization_124)
    block17_12_mixed = layers.Concatenate(axis=-1)([activation_121, activation_124])
    block17_12_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_12_conv_")(block17_12_mixed)
    block17_12_scale = layers.Lambda(lambda x: x * 0.100000)(block17_12_conv)
    block17_12 = layers.Add()([block17_11_ac, block17_12_scale])
    block17_12_ac = layers.ReLU()(block17_12)
    conv2d_126 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_126_")(block17_12_ac)
    batch_normalization_126 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_126_")(conv2d_126)
    activation_126 = layers.ReLU()(batch_normalization_126)
    conv2d_127 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_127_")(activation_126)
    batch_normalization_127 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_127_")(conv2d_127)
    activation_127 = layers.ReLU()(batch_normalization_127)
    conv2d_125 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_125_")(block17_12_ac)
    conv2d_128 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_128_")(activation_127)
    batch_normalization_125 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_125_")(conv2d_125)
    batch_normalization_128 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_128_")(conv2d_128)
    activation_125 = layers.ReLU()(batch_normalization_125)
    activation_128 = layers.ReLU()(batch_normalization_128)
    block17_13_mixed = layers.Concatenate(axis=-1)([activation_125, activation_128])
    block17_13_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_13_conv_")(block17_13_mixed)
    block17_13_scale = layers.Lambda(lambda x: x * 0.100000)(block17_13_conv)
    block17_13 = layers.Add()([block17_12_ac, block17_13_scale])
    block17_13_ac = layers.ReLU()(block17_13)
    conv2d_130 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_130_")(block17_13_ac)
    batch_normalization_130 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_130_")(conv2d_130)
    activation_130 = layers.ReLU()(batch_normalization_130)
    conv2d_131 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_131_")(activation_130)
    batch_normalization_131 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_131_")(conv2d_131)
    activation_131 = layers.ReLU()(batch_normalization_131)
    conv2d_129 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_129_")(block17_13_ac)
    conv2d_132 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_132_")(activation_131)
    batch_normalization_129 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_129_")(conv2d_129)
    batch_normalization_132 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_132_")(conv2d_132)
    activation_129 = layers.ReLU()(batch_normalization_129)
    activation_132 = layers.ReLU()(batch_normalization_132)
    block17_14_mixed = layers.Concatenate(axis=-1)([activation_129, activation_132])
    block17_14_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_14_conv_")(block17_14_mixed)
    block17_14_scale = layers.Lambda(lambda x: x * 0.100000)(block17_14_conv)
    block17_14 = layers.Add()([block17_13_ac, block17_14_scale])
    block17_14_ac = layers.ReLU()(block17_14)
    conv2d_134 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_134_")(block17_14_ac)
    batch_normalization_134 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_134_")(conv2d_134)
    activation_134 = layers.ReLU()(batch_normalization_134)
    conv2d_135 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_135_")(activation_134)
    batch_normalization_135 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_135_")(conv2d_135)
    activation_135 = layers.ReLU()(batch_normalization_135)
    conv2d_133 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_133_")(block17_14_ac)
    conv2d_136 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_136_")(activation_135)
    batch_normalization_133 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_133_")(conv2d_133)
    batch_normalization_136 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_136_")(conv2d_136)
    activation_133 = layers.ReLU()(batch_normalization_133)
    activation_136 = layers.ReLU()(batch_normalization_136)
    block17_15_mixed = layers.Concatenate(axis=-1)([activation_133, activation_136])
    block17_15_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_15_conv_")(block17_15_mixed)
    block17_15_scale = layers.Lambda(lambda x: x * 0.100000)(block17_15_conv)
    block17_15 = layers.Add()([block17_14_ac, block17_15_scale])
    block17_15_ac = layers.ReLU()(block17_15)
    conv2d_138 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_138_")(block17_15_ac)
    batch_normalization_138 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_138_")(conv2d_138)
    activation_138 = layers.ReLU()(batch_normalization_138)
    conv2d_139 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_139_")(activation_138)
    batch_normalization_139 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_139_")(conv2d_139)
    activation_139 = layers.ReLU()(batch_normalization_139)
    conv2d_137 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_137_")(block17_15_ac)
    conv2d_140 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_140_")(activation_139)
    batch_normalization_137 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_137_")(conv2d_137)
    batch_normalization_140 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_140_")(conv2d_140)
    activation_137 = layers.ReLU()(batch_normalization_137)
    activation_140 = layers.ReLU()(batch_normalization_140)
    block17_16_mixed = layers.Concatenate(axis=-1)([activation_137, activation_140])
    block17_16_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_16_conv_")(block17_16_mixed)
    block17_16_scale = layers.Lambda(lambda x: x * 0.100000)(block17_16_conv)
    block17_16 = layers.Add()([block17_15_ac, block17_16_scale])
    block17_16_ac = layers.ReLU()(block17_16)
    conv2d_142 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_142_")(block17_16_ac)
    batch_normalization_142 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_142_")(conv2d_142)
    activation_142 = layers.ReLU()(batch_normalization_142)
    conv2d_143 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_143_")(activation_142)
    batch_normalization_143 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_143_")(conv2d_143)
    activation_143 = layers.ReLU()(batch_normalization_143)
    conv2d_141 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_141_")(block17_16_ac)
    conv2d_144 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_144_")(activation_143)
    batch_normalization_141 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_141_")(conv2d_141)
    batch_normalization_144 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_144_")(conv2d_144)
    activation_141 = layers.ReLU()(batch_normalization_141)
    activation_144 = layers.ReLU()(batch_normalization_144)
    block17_17_mixed = layers.Concatenate(axis=-1)([activation_141, activation_144])
    block17_17_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_17_conv_")(block17_17_mixed)
    block17_17_scale = layers.Lambda(lambda x: x * 0.100000)(block17_17_conv)
    block17_17 = layers.Add()([block17_16_ac, block17_17_scale])
    block17_17_ac = layers.ReLU()(block17_17)
    conv2d_146 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_146_")(block17_17_ac)
    batch_normalization_146 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_146_")(conv2d_146)
    activation_146 = layers.ReLU()(batch_normalization_146)
    conv2d_147 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_147_")(activation_146)
    batch_normalization_147 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_147_")(conv2d_147)
    activation_147 = layers.ReLU()(batch_normalization_147)
    conv2d_145 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_145_")(block17_17_ac)
    conv2d_148 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_148_")(activation_147)
    batch_normalization_145 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_145_")(conv2d_145)
    batch_normalization_148 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_148_")(conv2d_148)
    activation_145 = layers.ReLU()(batch_normalization_145)
    activation_148 = layers.ReLU()(batch_normalization_148)
    block17_18_mixed = layers.Concatenate(axis=-1)([activation_145, activation_148])
    block17_18_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_18_conv_")(block17_18_mixed)
    block17_18_scale = layers.Lambda(lambda x: x * 0.100000)(block17_18_conv)
    block17_18 = layers.Add()([block17_17_ac, block17_18_scale])
    block17_18_ac = layers.ReLU()(block17_18)
    conv2d_150 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_150_")(block17_18_ac)
    batch_normalization_150 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_150_")(conv2d_150)
    activation_150 = layers.ReLU()(batch_normalization_150)
    conv2d_151 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_151_")(activation_150)
    batch_normalization_151 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_151_")(conv2d_151)
    activation_151 = layers.ReLU()(batch_normalization_151)
    conv2d_149 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_149_")(block17_18_ac)
    conv2d_152 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_152_")(activation_151)
    batch_normalization_149 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_149_")(conv2d_149)
    batch_normalization_152 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_152_")(conv2d_152)
    activation_149 = layers.ReLU()(batch_normalization_149)
    activation_152 = layers.ReLU()(batch_normalization_152)
    block17_19_mixed = layers.Concatenate(axis=-1)([activation_149, activation_152])
    block17_19_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_19_conv_")(block17_19_mixed)
    block17_19_scale = layers.Lambda(lambda x: x * 0.100000)(block17_19_conv)
    block17_19 = layers.Add()([block17_18_ac, block17_19_scale])
    block17_19_ac = layers.ReLU()(block17_19)
    conv2d_154 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_154_")(block17_19_ac)
    batch_normalization_154 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_154_")(conv2d_154)
    activation_154 = layers.ReLU()(batch_normalization_154)
    conv2d_155 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_155_")(activation_154)
    batch_normalization_155 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_155_")(conv2d_155)
    activation_155 = layers.ReLU()(batch_normalization_155)
    conv2d_153 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_153_")(block17_19_ac)
    conv2d_156 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_156_")(activation_155)
    batch_normalization_153 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_153_")(conv2d_153)
    batch_normalization_156 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_156_")(conv2d_156)
    activation_153 = layers.ReLU()(batch_normalization_153)
    activation_156 = layers.ReLU()(batch_normalization_156)
    block17_20_mixed = layers.Concatenate(axis=-1)([activation_153, activation_156])
    block17_20_conv = layers.Conv2D(1088, (1,1), padding="same", name="block17_20_conv_")(block17_20_mixed)
    block17_20_scale = layers.Lambda(lambda x: x * 0.100000)(block17_20_conv)
    block17_20 = layers.Add()([block17_19_ac, block17_20_scale])
    block17_20_ac = layers.ReLU()(block17_20)
    conv2d_161 = layers.Conv2D(256, (1,1), padding="same", name="conv2d_161_")(block17_20_ac)
    batch_normalization_161 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_161_")(conv2d_161)
    activation_161 = layers.ReLU()(batch_normalization_161)
    conv2d_157 = layers.Conv2D(256, (1,1), padding="same", name="conv2d_157_")(block17_20_ac)
    conv2d_159 = layers.Conv2D(256, (1,1), padding="same", name="conv2d_159_")(block17_20_ac)
    conv2d_162 = layers.Conv2D(288, (3,3), padding="same", name="conv2d_162_")(activation_161)
    batch_normalization_157 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_157_")(conv2d_157)
    batch_normalization_159 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_159_")(conv2d_159)
    batch_normalization_162 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_162_")(conv2d_162)
    activation_157 = layers.ReLU()(batch_normalization_157)
    activation_159 = layers.ReLU()(batch_normalization_159)
    activation_162 = layers.ReLU()(batch_normalization_162)
    conv2d_158 = layers.Conv2D(384, (3,3), strides=(2,2), name="conv2d_158_")(activation_157)
    conv2d_160 = layers.Conv2D(288, (3,3), strides=(2,2), name="conv2d_160_")(activation_159)
    conv2d_163 = layers.Conv2D(320, (3,3), strides=(2,2), name="conv2d_163_")(activation_162)
    batch_normalization_158 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_158_")(conv2d_158)
    batch_normalization_160 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_160_")(conv2d_160)
    batch_normalization_163 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_163_")(conv2d_163)
    activation_158 = layers.ReLU()(batch_normalization_158)
    activation_160 = layers.ReLU()(batch_normalization_160)
    activation_163 = layers.ReLU()(batch_normalization_163)
    max_pooling2d_4 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(block17_20_ac)
    mixed_7a = layers.Concatenate(axis=-1)([activation_158, activation_160, activation_163, max_pooling2d_4])
    conv2d_165 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_165_")(mixed_7a)
    batch_normalization_165 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_165_")(conv2d_165)
    activation_165 = layers.ReLU()(batch_normalization_165)
    conv2d_166 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_166_")(activation_165)
    batch_normalization_166 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_166_")(conv2d_166)
    activation_166 = layers.ReLU()(batch_normalization_166)
    conv2d_164 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_164_")(mixed_7a)
    conv2d_167 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_167_")(activation_166)
    batch_normalization_164 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_164_")(conv2d_164)
    batch_normalization_167 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_167_")(conv2d_167)
    activation_164 = layers.ReLU()(batch_normalization_164)
    activation_167 = layers.ReLU()(batch_normalization_167)
    block8_1_mixed = layers.Concatenate(axis=-1)([activation_164, activation_167])
    block8_1_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_1_conv_")(block8_1_mixed)
    block8_1_scale = layers.Lambda(lambda x: x * 0.200000)(block8_1_conv)
    block8_1 = layers.Add()([mixed_7a, block8_1_scale])
    block8_1_ac = layers.ReLU()(block8_1)
    conv2d_169 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_169_")(block8_1_ac)
    batch_normalization_169 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_169_")(conv2d_169)
    activation_169 = layers.ReLU()(batch_normalization_169)
    conv2d_170 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_170_")(activation_169)
    batch_normalization_170 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_170_")(conv2d_170)
    activation_170 = layers.ReLU()(batch_normalization_170)
    conv2d_168 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_168_")(block8_1_ac)
    conv2d_171 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_171_")(activation_170)
    batch_normalization_168 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_168_")(conv2d_168)
    batch_normalization_171 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_171_")(conv2d_171)
    activation_168 = layers.ReLU()(batch_normalization_168)
    activation_171 = layers.ReLU()(batch_normalization_171)
    block8_2_mixed = layers.Concatenate(axis=-1)([activation_168, activation_171])
    block8_2_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_2_conv_")(block8_2_mixed)
    block8_2_scale = layers.Lambda(lambda x: x * 0.200000)(block8_2_conv)
    block8_2 = layers.Add()([block8_1_ac, block8_2_scale])
    block8_2_ac = layers.ReLU()(block8_2)
    conv2d_173 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_173_")(block8_2_ac)
    batch_normalization_173 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_173_")(conv2d_173)
    activation_173 = layers.ReLU()(batch_normalization_173)
    conv2d_174 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_174_")(activation_173)
    batch_normalization_174 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_174_")(conv2d_174)
    activation_174 = layers.ReLU()(batch_normalization_174)
    conv2d_172 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_172_")(block8_2_ac)
    conv2d_175 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_175_")(activation_174)
    batch_normalization_172 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_172_")(conv2d_172)
    batch_normalization_175 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_175_")(conv2d_175)
    activation_172 = layers.ReLU()(batch_normalization_172)
    activation_175 = layers.ReLU()(batch_normalization_175)
    block8_3_mixed = layers.Concatenate(axis=-1)([activation_172, activation_175])
    block8_3_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_3_conv_")(block8_3_mixed)
    block8_3_scale = layers.Lambda(lambda x: x * 0.200000)(block8_3_conv)
    block8_3 = layers.Add()([block8_2_ac, block8_3_scale])
    block8_3_ac = layers.ReLU()(block8_3)
    conv2d_177 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_177_")(block8_3_ac)
    batch_normalization_177 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_177_")(conv2d_177)
    activation_177 = layers.ReLU()(batch_normalization_177)
    conv2d_178 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_178_")(activation_177)
    batch_normalization_178 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_178_")(conv2d_178)
    activation_178 = layers.ReLU()(batch_normalization_178)
    conv2d_176 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_176_")(block8_3_ac)
    conv2d_179 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_179_")(activation_178)
    batch_normalization_176 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_176_")(conv2d_176)
    batch_normalization_179 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_179_")(conv2d_179)
    activation_176 = layers.ReLU()(batch_normalization_176)
    activation_179 = layers.ReLU()(batch_normalization_179)
    block8_4_mixed = layers.Concatenate(axis=-1)([activation_176, activation_179])
    block8_4_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_4_conv_")(block8_4_mixed)
    block8_4_scale = layers.Lambda(lambda x: x * 0.200000)(block8_4_conv)
    block8_4 = layers.Add()([block8_3_ac, block8_4_scale])
    block8_4_ac = layers.ReLU()(block8_4)
    conv2d_181 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_181_")(block8_4_ac)
    batch_normalization_181 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_181_")(conv2d_181)
    activation_181 = layers.ReLU()(batch_normalization_181)
    conv2d_182 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_182_")(activation_181)
    batch_normalization_182 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_182_")(conv2d_182)
    activation_182 = layers.ReLU()(batch_normalization_182)
    conv2d_180 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_180_")(block8_4_ac)
    conv2d_183 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_183_")(activation_182)
    batch_normalization_180 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_180_")(conv2d_180)
    batch_normalization_183 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_183_")(conv2d_183)
    activation_180 = layers.ReLU()(batch_normalization_180)
    activation_183 = layers.ReLU()(batch_normalization_183)
    block8_5_mixed = layers.Concatenate(axis=-1)([activation_180, activation_183])
    block8_5_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_5_conv_")(block8_5_mixed)
    block8_5_scale = layers.Lambda(lambda x: x * 0.200000)(block8_5_conv)
    block8_5 = layers.Add()([block8_4_ac, block8_5_scale])
    block8_5_ac = layers.ReLU()(block8_5)
    conv2d_185 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_185_")(block8_5_ac)
    batch_normalization_185 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_185_")(conv2d_185)
    activation_185 = layers.ReLU()(batch_normalization_185)
    conv2d_186 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_186_")(activation_185)
    batch_normalization_186 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_186_")(conv2d_186)
    activation_186 = layers.ReLU()(batch_normalization_186)
    conv2d_184 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_184_")(block8_5_ac)
    conv2d_187 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_187_")(activation_186)
    batch_normalization_184 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_184_")(conv2d_184)
    batch_normalization_187 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_187_")(conv2d_187)
    activation_184 = layers.ReLU()(batch_normalization_184)
    activation_187 = layers.ReLU()(batch_normalization_187)
    block8_6_mixed = layers.Concatenate(axis=-1)([activation_184, activation_187])
    block8_6_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_6_conv_")(block8_6_mixed)
    block8_6_scale = layers.Lambda(lambda x: x * 0.200000)(block8_6_conv)
    block8_6 = layers.Add()([block8_5_ac, block8_6_scale])
    block8_6_ac = layers.ReLU()(block8_6)
    conv2d_189 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_189_")(block8_6_ac)
    batch_normalization_189 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_189_")(conv2d_189)
    activation_189 = layers.ReLU()(batch_normalization_189)
    conv2d_190 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_190_")(activation_189)
    batch_normalization_190 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_190_")(conv2d_190)
    activation_190 = layers.ReLU()(batch_normalization_190)
    conv2d_188 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_188_")(block8_6_ac)
    conv2d_191 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_191_")(activation_190)
    batch_normalization_188 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_188_")(conv2d_188)
    batch_normalization_191 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_191_")(conv2d_191)
    activation_188 = layers.ReLU()(batch_normalization_188)
    activation_191 = layers.ReLU()(batch_normalization_191)
    block8_7_mixed = layers.Concatenate(axis=-1)([activation_188, activation_191])
    block8_7_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_7_conv_")(block8_7_mixed)
    block8_7_scale = layers.Lambda(lambda x: x * 0.200000)(block8_7_conv)
    block8_7 = layers.Add()([block8_6_ac, block8_7_scale])
    block8_7_ac = layers.ReLU()(block8_7)
    conv2d_193 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_193_")(block8_7_ac)
    batch_normalization_193 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_193_")(conv2d_193)
    activation_193 = layers.ReLU()(batch_normalization_193)
    conv2d_194 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_194_")(activation_193)
    batch_normalization_194 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_194_")(conv2d_194)
    activation_194 = layers.ReLU()(batch_normalization_194)
    conv2d_192 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_192_")(block8_7_ac)
    conv2d_195 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_195_")(activation_194)
    batch_normalization_192 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_192_")(conv2d_192)
    batch_normalization_195 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_195_")(conv2d_195)
    activation_192 = layers.ReLU()(batch_normalization_192)
    activation_195 = layers.ReLU()(batch_normalization_195)
    block8_8_mixed = layers.Concatenate(axis=-1)([activation_192, activation_195])
    block8_8_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_8_conv_")(block8_8_mixed)
    block8_8_scale = layers.Lambda(lambda x: x * 0.200000)(block8_8_conv)
    block8_8 = layers.Add()([block8_7_ac, block8_8_scale])
    block8_8_ac = layers.ReLU()(block8_8)
    conv2d_197 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_197_")(block8_8_ac)
    batch_normalization_197 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_197_")(conv2d_197)
    activation_197 = layers.ReLU()(batch_normalization_197)
    conv2d_198 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_198_")(activation_197)
    batch_normalization_198 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_198_")(conv2d_198)
    activation_198 = layers.ReLU()(batch_normalization_198)
    conv2d_196 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_196_")(block8_8_ac)
    conv2d_199 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_199_")(activation_198)
    batch_normalization_196 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_196_")(conv2d_196)
    batch_normalization_199 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_199_")(conv2d_199)
    activation_196 = layers.ReLU()(batch_normalization_196)
    activation_199 = layers.ReLU()(batch_normalization_199)
    block8_9_mixed = layers.Concatenate(axis=-1)([activation_196, activation_199])
    block8_9_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_9_conv_")(block8_9_mixed)
    block8_9_scale = layers.Lambda(lambda x: x * 0.200000)(block8_9_conv)
    block8_9 = layers.Add()([block8_8_ac, block8_9_scale])
    block8_9_ac = layers.ReLU()(block8_9)
    conv2d_201 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_201_")(block8_9_ac)
    batch_normalization_201 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_201_")(conv2d_201)
    activation_201 = layers.ReLU()(batch_normalization_201)
    conv2d_202 = layers.Conv2D(224, (1,3), padding="same", name="conv2d_202_")(activation_201)
    batch_normalization_202 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_202_")(conv2d_202)
    activation_202 = layers.ReLU()(batch_normalization_202)
    conv2d_200 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_200_")(block8_9_ac)
    conv2d_203 = layers.Conv2D(256, (3,1), padding="same", name="conv2d_203_")(activation_202)
    batch_normalization_200 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_200_")(conv2d_200)
    batch_normalization_203 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_203_")(conv2d_203)
    activation_200 = layers.ReLU()(batch_normalization_200)
    activation_203 = layers.ReLU()(batch_normalization_203)
    block8_10_mixed = layers.Concatenate(axis=-1)([activation_200, activation_203])
    block8_10_conv = layers.Conv2D(2080, (1,1), padding="same", name="block8_10_conv_")(block8_10_mixed)
    block8_10_scale = layers.Lambda(lambda x: x * 1.000000)(block8_10_conv)
    block8_10 = layers.Add()([block8_9_ac, block8_10_scale])
    conv_7b = layers.Conv2D(1536, (1,1), padding="same", name="conv_7b_")(block8_10)
    conv_7b_bn = layers.BatchNormalization(epsilon=0.001000, name="conv_7b_bn_")(conv_7b)
    conv_7b_ac = layers.ReLU()(conv_7b_bn)
    avg_pool = layers.GlobalAveragePooling2D(keepdims=True)(conv_7b_ac)
    new_dropout = layers.Dropout(0.300000)(avg_pool)
    new_fc = layers.Reshape((-1,), name="new_fc_preFlatten1")(new_dropout)
    new_fc = layers.Dense(13, name="new_fc_")(new_fc)
    new_softmax = layers.Softmax()(new_fc)

    model = keras.Model(inputs=[input_1_unnormalized], outputs=[new_softmax])
    return model
